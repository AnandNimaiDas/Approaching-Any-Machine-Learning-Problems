{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "When you are done creating hundreds of thousands of features, it’s time for selecting a few of them. Well, we should never create hundreds of thousands of useless features. Having too many features pose a problem well known as the **curse of dimensionality.** If you have a lot of features, you must also have a lot of training samples to capture all the features. What’s considered a “lot” is not defined correctly and is up to you to figure out by validating your models properly and checking how much time it takes to train your models. \n",
    "\n",
    "\n",
    "The simplest form of selecting features would be to remove features with very low variance. If the features have a very low variance (i.e. very close to 0), they are close to being constant and thus, do not add any value to any model at all. It would just be nice to get rid of them and hence lower the complexity. Please note that the variance also depends on scaling of the data. Scikit-learn has an implementation for VarianceThreshold that does precisely this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold \n",
    "\n",
    "data =... \n",
    "var_thresh = VarianceThreshold(threshold=0.1) \n",
    "transformed_data = var_thresh.fit_transform(data)\n",
    "\n",
    "#transformed data will have all columns with variance less \n",
    "#than 0.1 removed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can aslo remove features whcih have a high correlation. For calucalting the correation between differetn numiercal feautres, you can use the **Pearson correlation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedInc_Sqrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MedInc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119034</td>\n",
       "      <td>0.326895</td>\n",
       "      <td>-0.062040</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.018766</td>\n",
       "      <td>-0.079809</td>\n",
       "      <td>-0.015176</td>\n",
       "      <td>0.984329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HouseAge</th>\n",
       "      <td>-0.119034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.153277</td>\n",
       "      <td>-0.077747</td>\n",
       "      <td>-0.296244</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>-0.108197</td>\n",
       "      <td>-0.132797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveRooms</th>\n",
       "      <td>0.326895</td>\n",
       "      <td>-0.153277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847621</td>\n",
       "      <td>-0.072213</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>0.106389</td>\n",
       "      <td>-0.027540</td>\n",
       "      <td>0.326688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveBedrms</th>\n",
       "      <td>-0.062040</td>\n",
       "      <td>-0.077747</td>\n",
       "      <td>0.847621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066197</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>-0.066910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population</th>\n",
       "      <td>0.004834</td>\n",
       "      <td>-0.296244</td>\n",
       "      <td>-0.072213</td>\n",
       "      <td>-0.066197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069863</td>\n",
       "      <td>-0.108785</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.018415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveOccup</th>\n",
       "      <td>0.018766</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>0.069863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.015266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>-0.079809</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.106389</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>-0.108785</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.924664</td>\n",
       "      <td>-0.084303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>-0.015176</td>\n",
       "      <td>-0.108197</td>\n",
       "      <td>-0.027540</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>-0.924664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedInc_Sqrt</th>\n",
       "      <td>0.984329</td>\n",
       "      <td>-0.132797</td>\n",
       "      <td>0.326688</td>\n",
       "      <td>-0.066910</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>0.015266</td>\n",
       "      <td>-0.084303</td>\n",
       "      <td>-0.015569</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
       "MedInc       1.000000 -0.119034  0.326895  -0.062040    0.004834  0.018766   \n",
       "HouseAge    -0.119034  1.000000 -0.153277  -0.077747   -0.296244  0.013191   \n",
       "AveRooms     0.326895 -0.153277  1.000000   0.847621   -0.072213 -0.004852   \n",
       "AveBedrms   -0.062040 -0.077747  0.847621   1.000000   -0.066197 -0.006181   \n",
       "Population   0.004834 -0.296244 -0.072213  -0.066197    1.000000  0.069863   \n",
       "AveOccup     0.018766  0.013191 -0.004852  -0.006181    0.069863  1.000000   \n",
       "Latitude    -0.079809  0.011173  0.106389   0.069721   -0.108785  0.002366   \n",
       "Longitude   -0.015176 -0.108197 -0.027540   0.013344    0.099773  0.002476   \n",
       "MedInc_Sqrt  0.984329 -0.132797  0.326688  -0.066910    0.018415  0.015266   \n",
       "\n",
       "             Latitude  Longitude  MedInc_Sqrt  \n",
       "MedInc      -0.079809  -0.015176     0.984329  \n",
       "HouseAge     0.011173  -0.108197    -0.132797  \n",
       "AveRooms     0.106389  -0.027540     0.326688  \n",
       "AveBedrms    0.069721   0.013344    -0.066910  \n",
       "Population  -0.108785   0.099773     0.018415  \n",
       "AveOccup     0.002366   0.002476     0.015266  \n",
       "Latitude     1.000000  -0.924664    -0.084303  \n",
       "Longitude   -0.924664   1.000000    -0.015569  \n",
       "MedInc_Sqrt -0.084303  -0.015569     1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing \n",
    "\n",
    "#fetch a regression dataset \n",
    "data = fetch_california_housing() \n",
    "x = data[\"data\"]\n",
    "\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]\n",
    "\n",
    "#convert to pandas dataframe \n",
    "df = pd.DataFrame(x,columns = col_names)\n",
    "\n",
    "#introduce a highly correlated column \n",
    "df.loc[:,\"MedInc_Sqrt\"] = df.MedInc.apply(np.sqrt)\n",
    "\n",
    "#get correlation matrix(pearson)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In The above table we can see that the **MedIn_Sqrt** has a very correlation with *MedInc*. We can thus remove one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can move to some univariate ways of feature selection. **Univariate feature selection** is nothing but a scoring of each feature against a given target. **Mutual information, ANOVA F-test** and **chi2** are some of the most popular methods for univariate feature selection. There are two ways of using these in scikitlearn.  \n",
    "*  SelectKBest: It keeps the top-k scoring features \n",
    "* SelectPercentile: It keeps the top features which are in a percentage specified by the user \n",
    "\n",
    "It must be noted that you can use chi2 only for data which is non-negative in nature. This is a particularly useful feature selection technique in natural language processing when we have a bag of words or tf-idf based features. It’s best to create a wrapper for univariate feature selection that you can use for almost any new problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif \n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif \n",
    "from sklearn.feature_selection import mutual_info_regression \n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "class UnivariateFeatureSelction:\n",
    "    \n",
    "    def __init__(self, n_features, problem_type, scoring): \n",
    "        \"\"\"\n",
    "        Custom univariate feature selection wrapper on \n",
    "        different univariate feature selection models from \n",
    "        scikit-learn. :param n_features: SelectPercentile if float else \n",
    "        SelectKBest :param problem_type: classification or regression \n",
    "        :param scoring: scoring function, string \"\"\"\n",
    "        #for a given problem type, there are only \n",
    "        #a few valid scoring methods \n",
    "        #you can extend this with your own custom \n",
    "        #methods if you wish \n",
    "        if problem_type == \"classification\": \n",
    "            valid_scoring = { \"f_classif\": f_classif, \"chi2\": chi2, \"mutual_info_classif\": mutual_info_classif } \n",
    "        \n",
    "        else:\n",
    "            valid_scoring = { \"f_regression\": f_regression, \"mutual_info_regression\": mutual_info_regression } \n",
    "            \n",
    "        #raise exception if we do not have a valid scoring method \n",
    "        if scoring not in valid_scoring:\n",
    "            raise Exception(\"Invalid scoring function\") \n",
    "            \n",
    "        #if n_features is int, we use selectkbest \n",
    "        #if n_features is float, we use selectpercentile \n",
    "        #please note that it is int in both cases in sklearn \n",
    "        \n",
    "        if isinstance(n_features, int):\n",
    "            self.selection = SelectKBest( valid_scoring[scoring], \n",
    "                                         k=n_features) \n",
    "        \n",
    "        elif isinstance(n_features, float): \n",
    "            \n",
    "            self.selection = SelectPercentile( valid_scoring[scoring], percentile=int(n_features * 100) ) \n",
    "            \n",
    "        else: \n",
    "            raise Exception(\"Invalid type of feature\") \n",
    "            \n",
    "    #same fit function\n",
    "    def fit(self, X, y): \n",
    "        return self.selection.fit(X, y) \n",
    "    \n",
    "    #same transform function \n",
    "    def transform(self, X):\n",
    "        \n",
    "        return self.selection.transform(X) \n",
    "    \n",
    "    #same fit_transform function \n",
    "    def fit_transform(self, X, y): \n",
    "        \n",
    "        return self.selection.fit_transform(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufs = UnivariateFeatureSelction( \n",
    "    n_features=0.1, \n",
    "    problem_type=\"regression\", \n",
    "    scoring=\"f_regression\" ) \n",
    "\n",
    "ufs.fit(x, y) \n",
    "X_transformed = ufs.transform(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should take care of most of your univariate feature selection needs. Please note that it’s usually better to create less and important features than to create hundreds of features in the first place. Univariate feature selection may not always perform well. Most of the time, people prefer doing feature selection using a machine learning model. Let’s see how that is done. \n",
    "\n",
    "The simplest form of feature selection that uses a model for selection is known as **greedy feature selection.** In greedy feature selection, \n",
    "* the first step is to choose a model. \n",
    "* The second step is to select a loss/scoring function. \n",
    "* And the third and final step is to iteratively evaluate each feature and add it to the list of “good” features if it improves loss/score. \n",
    "\n",
    "It can’t get simpler than this. But you must keep in mind that this is known as greedy feature selection for a reason. This feature selection process will fit a given model each time it evaluates a feature. The computational cost associated with this kind of method is very high. It will also take a lot of time for this kind of feature selection to finish. And if you do not use this feature selection properly, then you might even end up overfitting the model.\n",
    "Let’s see how it works by looking at how its implemented. \n",
    "\n",
    "\n",
    "* The code for greedy algorithm is in **greedy.py** script and on running it we will get the following output.\n",
    "\n",
    "<img src=\"../feature_sel1.png\">\n",
    "\n",
    "\n",
    "Figure 2 shows how this score improves with the addition of a new feature in every iteration. We see that we are not able to improve our score after a certain point, and that’s where we stop. \n",
    "\n",
    "<img src=\"../feature_sel.jpeg\">\n",
    "\n",
    "\n",
    "Another greedy approach is known as **recursive feature elimination (RFE)**. In the previous method, we started with one feature and kept adding new features, but in RFE, we start with all features and keep removing one feature in every iteration that provides the least value to a given model. But how to do we know which feature offers the least value? Well, if we use models like linear support vector machine (SVM) or logistic regression, we get a coefficient for each feature which decides the importance of the features. In case of any tree-based models, we get feature importance in place of coefficients. In each iteration, we can eliminate the least important features and keep eliminating it until we reach the number of features needed. So, we have the ability to decide how many features we want to keep.\n",
    "\n",
    "When we are doing recursive feature elimination, in each iteration, we remove the feature which has the feature importance or the feature which has a coefficient close to 0. Please remember that when you use a model like logistic regression for binary classification, the coefficients for features are more positive if they are important for the positive class and more negative if they are important for the negative class. It’s very easy to modify our greedy feature selection class to create a new class for recursive feature elimination, but scikit-learn also provides RFE out of the box. \n",
    "\n",
    "* A simple usage is shown in **REF.py** script and on running the script the output is\n",
    "\n",
    "<img src=\"../feature_sel2.png\">\n",
    "\n",
    "\n",
    "We saw two different greedy ways to select features from a model. But you can also fit the model to the data and select features from the model by the feature coefficients or the importance of features. If you use coefficients, you can select a threshold, and if the coefficient is above that threshold, you can keep the feature else eliminate it. Let’s see how we can get feature importance from a model like random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import load_diabetes \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "#fetch a regression dataset \n",
    "#in diabetes data we predict diabetes progression \n",
    "#after one year based on some features \n",
    "data = load_diabetes() \n",
    "X = data[\"data\"] \n",
    "col_names = data[\"feature_names\"] \n",
    "y = data[\"target\"] \n",
    "\n",
    "#initialize the model \n",
    "model = RandomForestRegressor() \n",
    "\n",
    "#fit the model \n",
    "model.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "importances = model.feature_importances_ \n",
    "idxs = np.argsort(importances) \n",
    "plt.title('Feature Importances') \n",
    "plt.barh(range(len(idxs)), importances[idxs], align='center') \n",
    "plt.yticks(range(len(idxs)), [col_names[i] for i in idxs]) \n",
    "plt.xlabel('Random Forest Feature Importance') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, selecting the best features from the model is nothing new. You can choose features from one model and use another model to train. For example, you can use Logistic Regression coefficients to select the features and then use Random Forest to train the model on chosen features. Scikit-learn also offers SelectFromModel class that helps you choose features directly from a given model. You can also specify the threshold for coefficients or feature importance if you want and the maximum number of features you want to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bmi', 's5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "\n",
    "#fetch a regression dataset \n",
    "#in diabetes data we predict diabetes progression \n",
    "#after one year based on some features \n",
    "data = load_diabetes() \n",
    "\n",
    "X = data[\"data\"] \n",
    "col_names = data[\"feature_names\"] \n",
    "y = data[\"target\"] \n",
    "\n",
    "#initialize the model \n",
    "model = RandomForestRegressor() \n",
    "\n",
    "#select from the model\n",
    "sfm = SelectFromModel(estimator=model) \n",
    "X_transformed = sfm.fit_transform(X, y) \n",
    "#see which features were selected \n",
    "support = sfm.get_support() \n",
    "\n",
    "#get feature names \n",
    "print([ x for x, y in zip(col_names, support) if y == True ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which prints: ['bmi', 's5']. When we look at figure 3, we see that these are the top- 2 features. Thus, we could have also selected directly from feature importance provided by random forest. \n",
    "\n",
    "One more thing that we are missing here is feature selection using models that have **L1 (Lasso) penalization**. When we have L1 penalization for regularization, most coefficients will be 0 (or close to 0), and we select the features with non-zero coefficients. You can do it by just replacing random forest in the snippet of selection from a model with a model that supports L1 penalty, e.g. lasso regression. All tree-based models provide feature importance so all the model-based snippets shown in this chapter can be used for XGBoost, LightGBM or CatBoost. The feature importance function names might be different and may produce results in a different format, but the usage will remain the same. In the end, you must be careful when doing feature selection. Select features on training data and validate the model on validation data for proper selection of features without overfitting the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
