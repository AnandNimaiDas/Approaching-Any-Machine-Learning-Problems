{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "\n",
    "**Feature engineering is not about new features from data but also includes different types of normalization  and transformations.**\n",
    "\n",
    "Let us say that you are dealing with **date and time data**. So, we have a pandas dataframe with datetime column. Using thid column, we can create features like.\n",
    "\n",
    "* Year\n",
    "* Week of year\n",
    "* Month\n",
    "* Day of week\n",
    "* Weekend\n",
    "* Hour\n",
    "* And many more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The below code show how we can implement it.**\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "* df.loc[:,\"year\"] = df[\"date\"].dt.year\n",
    "* df.loc[:,\"weekofyear\"] = df[\"date\"].dt.weekofyear \n",
    "* df.loc[:,\"month\"] = df[\"date\"].dt.month\n",
    "* df.loc[:,\"dayofweek\"] = df[\"date\"].dt.dayofweek\n",
    "* df.loc[:,\"weekend\"] = (df[\"date\"].dt.weekday >= 5).astype(int)\n",
    "* df.loc[:,\"hour\"] = df[\"dateyime_column\"].dt.hour\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#create a series datetime with a frequency of 10 hrs\n",
    "s = pd.date_range('2020-01-06','2020-01-10',freq = '10H').to_series()\n",
    "\n",
    "#create some features baesd on datatime \n",
    "features = {\n",
    "    \"dayofweek\": s.dt.dayofweek.values,\n",
    "    \"dayofyear\": s.dt.dayofyear.values,\n",
    "    \"hour\": s.dt.hour.values,\n",
    "    \"is_leap_year\": s.dt.is_leap_year.values,\n",
    "    \"quarter\": s.dt.quarter.values,\n",
    "    \"weekofyear\": s.dt.weekofyear.values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dayofweek': array([0, 0, 0, 1, 1, 2, 2, 2, 3, 3]),\n",
       " 'dayofyear': array([6, 6, 6, 7, 7, 8, 8, 8, 9, 9]),\n",
       " 'hour': array([ 0, 10, 20,  6, 16,  2, 12, 22,  8, 18]),\n",
       " 'is_leap_year': array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]),\n",
       " 'quarter': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'weekofyear': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code generate a dictionary of features from a given series. \n",
    "* You can apply this to any datetime column in a pandas dataframe. These are some of the many date time features that pandas offer. \n",
    "* Date time features are critical when you are dealing with time-series data, for example, predicting sales of a store but would like to use a model like xgboost on aggregated features. \n",
    "\n",
    "\n",
    "Now lets create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"date\":['2016-09-01','2017-04-01','2017-08-01','2017-12-01','2017-09-01'],\n",
    "        \"customer_id\": [146361,180838,157857,159772,80014],\n",
    "         \"cart1\":[2,4,3,5,3],\n",
    "         \"cart2\": [2,1,3,1,2],\n",
    "          \"cart3\":[0,0,1,1,1],\n",
    "          \"num1\": [-0.518679,0.415853,-2.061687,-0.276558,-1.456827]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>cart1</th>\n",
       "      <th>cart2</th>\n",
       "      <th>cart3</th>\n",
       "      <th>num1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>146361</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.518679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>180838</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.415853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>157857</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.061687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>159772</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.276558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>80014</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.456827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  customer_id  cart1  cart2  cart3      num1\n",
       "0  2016-09-01       146361      2      2      0 -0.518679\n",
       "1  2017-04-01       180838      4      1      0  0.415853\n",
       "2  2017-08-01       157857      3      3      1 -2.061687\n",
       "3  2017-12-01       159772      5      1      1 -0.276558\n",
       "4  2017-09-01        80014      3      2      1 -1.456827"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the date object to datetime.\n",
    "new_data[\"date\"] = pd.to_datetime(new_data['date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using aggregate to create more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    \n",
    "    df.loc[:,\"year\"] = df[\"date\"].dt.year\n",
    "    df.loc[:,\"weekofyear\"] = df[\"date\"].dt.weekofyear \n",
    "    df.loc[:,\"month\"] = df[\"date\"].dt.month\n",
    "    df.loc[:,\"dayofweek\"] = df[\"date\"].dt.dayofweek\n",
    "    df.loc[:,\"weekend\"] = (df[\"date\"].dt.weekday >= 5).astype(int)\n",
    "    \n",
    "    #create an aggregate dictonary \n",
    "    \n",
    "    aggs = {} \n",
    "    #for aggreation by month, we calculate the\n",
    "    #number of unique month values and also the mean \n",
    "    aggs[\"month\"] = [\"nunique\",\"mean\"]\n",
    "    \n",
    "    #for aggreation by month, we calculate the\n",
    "    #number of unique month values nd also the mean\n",
    "    aggs[\"month\"] = [\"nunique\",\"mean\"]\n",
    "    aggs[\"weekofyear\"] = [\"nunique\",\"mean\"]\n",
    "    \n",
    "    #we aggregate by num1 and calculate sum,max,min \n",
    "    #and mean values of this column \n",
    "    aggs[\"num1\"] = [\"sum\",\"max\",\"min\",\"mean\"]\n",
    "    \n",
    "    #for customer_id, we calculate the total count \n",
    "    aggs[\"customer_id\"]  = [\"size\"]\n",
    "    \n",
    "    #again for customer_id, we calculate the total unqiue \n",
    "    aggs[\"customer_id\"] = [\"nunique\"]\n",
    "    \n",
    "    #we group by customer_id and calculate the aggregates \n",
    "    agg_df = df.groupby(\"customer_id\").agg(aggs)\n",
    "    agg_df = agg_df.reset_index() \n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data1 = generate_features(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th colspan=\"2\" halign=\"left\">month</th>\n",
       "      <th colspan=\"2\" halign=\"left\">weekofyear</th>\n",
       "      <th colspan=\"4\" halign=\"left\">num1</th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>nunique</th>\n",
       "      <th>mean</th>\n",
       "      <th>nunique</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "      <th>nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80014</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>-1.456827</td>\n",
       "      <td>-1.456827</td>\n",
       "      <td>-1.456827</td>\n",
       "      <td>-1.456827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146361</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.518679</td>\n",
       "      <td>-0.518679</td>\n",
       "      <td>-0.518679</td>\n",
       "      <td>-0.518679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157857</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>-2.061687</td>\n",
       "      <td>-2.061687</td>\n",
       "      <td>-2.061687</td>\n",
       "      <td>-2.061687</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159772</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.276558</td>\n",
       "      <td>-0.276558</td>\n",
       "      <td>-0.276558</td>\n",
       "      <td>-0.276558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180838</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.415853</td>\n",
       "      <td>0.415853</td>\n",
       "      <td>0.415853</td>\n",
       "      <td>0.415853</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id   month      weekofyear           num1                      \\\n",
       "              nunique mean    nunique mean       sum       max       min   \n",
       "0       80014       1    9          1   35 -1.456827 -1.456827 -1.456827   \n",
       "1      146361       1    9          1   35 -0.518679 -0.518679 -0.518679   \n",
       "2      157857       1    8          1   31 -2.061687 -2.061687 -2.061687   \n",
       "3      159772       1   12          1   48 -0.276558 -0.276558 -0.276558   \n",
       "4      180838       1    4          1   13  0.415853  0.415853  0.415853   \n",
       "\n",
       "            customer_id  \n",
       "       mean     nunique  \n",
       "0 -1.456827           1  \n",
       "1 -0.518679           1  \n",
       "2 -2.061687           1  \n",
       "3 -0.276558           1  \n",
       "4  0.415853           1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we will get the above features after using aggreagate.\n",
    "\n",
    "Here we are not trying to predcit anything, we are just creating generic features. However, it would have been easier to create features if we were trying to predict something here.\n",
    "\n",
    "Sometimes, for example, when dealing with time-series problems, you might have features which are not individual values but a list of values. For example, transactions by a customer in a given period of time. In these cases, we create different types of features such as: with numerical features, when you are grouping on a categorical column, you will get features like a list of values which are time distributed. In these cases, you can create a bunch of statistical features such as:\n",
    "\n",
    "* Mean\n",
    "* Max\n",
    "* Unique\n",
    "* Skew\n",
    "* Kurtosis\n",
    "* Kstat \n",
    "* Percentile\n",
    "* Quantile\n",
    "* Peak to peak\n",
    "* And many more \n",
    "\n",
    "This can be created using numpy functions as shown in the following  python snippet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "feature_dict = {} \n",
    "\n",
    "#calculate mean \n",
    "feature_dict['mean'] = np.mean(x) \n",
    "\n",
    "#calculate max \n",
    "feature_dict['max'] = np.max(x) \n",
    "\n",
    "#calculate min \n",
    "feature_dict['min'] = np.min(x) \n",
    "\n",
    "#calculate standard deviation \n",
    "feature_dict['std'] = np.std(x) \n",
    "\n",
    "#calculate variance \n",
    "feature_dict['var'] = np.var(x) \n",
    "\n",
    "#peak-to-peak \n",
    "feature_dict['ptp'] = np.ptp(x) \n",
    "\n",
    "#percentile features \n",
    "feature_dict['percentile_10'] = np.percentile(x, 10) \n",
    "feature_dict['percentile_60'] = np.percentile(x, 60) \n",
    "feature_dict['percentile_90'] = np.percentile(x, 90) \n",
    "\n",
    "#quantile features \n",
    "feature_dict['quantile_5'] = np.percentile(x, 5) \n",
    "feature_dict['quantile_95'] = np.percentile(x, 95) \n",
    "feature_dict['quantile_99'] = np.percentile(x, 99) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series data(list of values) can be converted to a lot of features.\n",
    "\n",
    "A python library called **tsfresh** is instrumental in this cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.feature_extraction import feature_calculators as fc \n",
    "\n",
    "#tsfresh based features \n",
    "feature_dict['abs_energy'] = fc.abs_energy(x) \n",
    "feature_dict['count_above_mean'] = fc.count_above_mean(x) \n",
    "feature_dict['count_below_mean'] = fc.count_below_mean(x) \n",
    "feature_dict['mean_abs_change'] = fc.mean_abs_change(x) \n",
    "feature_dict['mean_change'] = fc.mean_change(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not all; tsfresh offers hundreds of features and tens of variations of different features that you can use for time series (list of values) based features. In the examples above, x is a list of values. But that’s not all. There are many other features that you can create for numerical data with or without categorical data. A simple way to generate many features is just to create a bunch of **polynomial features.** For example, a second-degree polynomial feature from two features “a” and “b” would include: “a”, “b”, “ab”, “a2 ” and “b2 ”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "#generate a random dataframe with \n",
    "#2 columns and 100 rows \n",
    "df = pd.DataFrame(\n",
    "    np.random.rand(100,2),\n",
    " columns = [f\"f_{i}\" for i in range(1,3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924478</td>\n",
       "      <td>0.316386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.876845</td>\n",
       "      <td>0.442913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.861783</td>\n",
       "      <td>0.113082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.397906</td>\n",
       "      <td>0.658547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178985</td>\n",
       "      <td>0.953942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.158267</td>\n",
       "      <td>0.619018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.065997</td>\n",
       "      <td>0.767432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.755679</td>\n",
       "      <td>0.614073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.724800</td>\n",
       "      <td>0.048357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.315393</td>\n",
       "      <td>0.981535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f_1       f_2\n",
       "0  0.924478  0.316386\n",
       "1  0.876845  0.442913\n",
       "2  0.861783  0.113082\n",
       "3  0.397906  0.658547\n",
       "4  0.178985  0.953942\n",
       "5  0.158267  0.619018\n",
       "6  0.065997  0.767432\n",
       "7  0.755679  0.614073\n",
       "8  0.724800  0.048357\n",
       "9  0.315393  0.981535"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create two-degree polynomial feautres using PolynomialFeatures from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "\n",
    "#initialize polynomial features class object \n",
    "#for two-degree polynomial features \n",
    "\n",
    "pf = preprocessing.PolynomialFeatures( degree=2, interaction_only=False, include_bias=False ) \n",
    "\n",
    "#fit to the features \n",
    "pf.fit(df) \n",
    "\n",
    "#create polynomial features \n",
    "poly_feats = pf.transform(df) \n",
    "\n",
    "#create a dataframe with all the features \n",
    "num_feats = poly_feats.shape[1] \n",
    "df_transformed = pd.DataFrame( poly_feats, columns=[f\"f_{i}\" for i in range(1, num_feats + 1)] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924478</td>\n",
       "      <td>0.316386</td>\n",
       "      <td>0.854659</td>\n",
       "      <td>0.292492</td>\n",
       "      <td>0.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.876845</td>\n",
       "      <td>0.442913</td>\n",
       "      <td>0.768856</td>\n",
       "      <td>0.388366</td>\n",
       "      <td>0.196172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.861783</td>\n",
       "      <td>0.113082</td>\n",
       "      <td>0.742671</td>\n",
       "      <td>0.097453</td>\n",
       "      <td>0.012788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.397906</td>\n",
       "      <td>0.658547</td>\n",
       "      <td>0.158329</td>\n",
       "      <td>0.262040</td>\n",
       "      <td>0.433685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178985</td>\n",
       "      <td>0.953942</td>\n",
       "      <td>0.032036</td>\n",
       "      <td>0.170741</td>\n",
       "      <td>0.910006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.158267</td>\n",
       "      <td>0.619018</td>\n",
       "      <td>0.025048</td>\n",
       "      <td>0.097970</td>\n",
       "      <td>0.383184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.065997</td>\n",
       "      <td>0.767432</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.050648</td>\n",
       "      <td>0.588952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.755679</td>\n",
       "      <td>0.614073</td>\n",
       "      <td>0.571051</td>\n",
       "      <td>0.464042</td>\n",
       "      <td>0.377085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.724800</td>\n",
       "      <td>0.048357</td>\n",
       "      <td>0.525336</td>\n",
       "      <td>0.035049</td>\n",
       "      <td>0.002338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.315393</td>\n",
       "      <td>0.981535</td>\n",
       "      <td>0.099473</td>\n",
       "      <td>0.309570</td>\n",
       "      <td>0.963411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f_1       f_2       f_3       f_4       f_5\n",
       "0  0.924478  0.316386  0.854659  0.292492  0.100100\n",
       "1  0.876845  0.442913  0.768856  0.388366  0.196172\n",
       "2  0.861783  0.113082  0.742671  0.097453  0.012788\n",
       "3  0.397906  0.658547  0.158329  0.262040  0.433685\n",
       "4  0.178985  0.953942  0.032036  0.170741  0.910006\n",
       "5  0.158267  0.619018  0.025048  0.097970  0.383184\n",
       "6  0.065997  0.767432  0.004356  0.050648  0.588952\n",
       "7  0.755679  0.614073  0.571051  0.464042  0.377085\n",
       "8  0.724800  0.048357  0.525336  0.035049  0.002338\n",
       "9  0.315393  0.981535  0.099473  0.309570  0.963411"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create third-degree polynomial features then there will be nine features in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"feature_eng1.jpeg\">\n",
    "\n",
    "Another interesting features converts the numbers to categories.It's known as **bining**. Let's look figure 5, which shows a sample histogram of random numerical features. We use ten bins for this figure, and we see that we can divide the data  into ten parts. This is  accomplised using the pandas *cut* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bins of the numercial columns \n",
    "# 10 bins \n",
    "df[\"f_bin_10\"] = pd.cut(df[\"f_1\"],bins = 10,labels = False)\n",
    "df[\"f_bin_100\"] = pd.cut(df[\"f_1\"],bins = 100,labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_bin_10</th>\n",
       "      <th>f_bin_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924478</td>\n",
       "      <td>0.316386</td>\n",
       "      <td>9</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.876845</td>\n",
       "      <td>0.442913</td>\n",
       "      <td>8</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.861783</td>\n",
       "      <td>0.113082</td>\n",
       "      <td>8</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.397906</td>\n",
       "      <td>0.658547</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178985</td>\n",
       "      <td>0.953942</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.158267</td>\n",
       "      <td>0.619018</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.065997</td>\n",
       "      <td>0.767432</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.755679</td>\n",
       "      <td>0.614073</td>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.724800</td>\n",
       "      <td>0.048357</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.315393</td>\n",
       "      <td>0.981535</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f_1       f_2  f_bin_10  f_bin_100\n",
       "0  0.924478  0.316386         9         92\n",
       "1  0.876845  0.442913         8         88\n",
       "2  0.861783  0.113082         8         86\n",
       "3  0.397906  0.658547         3         39\n",
       "4  0.178985  0.953942         1         17\n",
       "5  0.158267  0.619018         1         14\n",
       "6  0.065997  0.767432         0          5\n",
       "7  0.755679  0.614073         7         75\n",
       "8  0.724800  0.048357         7         72\n",
       "9  0.315393  0.981535         3         30"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bining enables you to treat numerical features as categorical**\n",
    "\n",
    "Yet another interesting type of features that you can create from numerical features is log transformation.\n",
    "Take a look a given table below. \n",
    "\n",
    "<img src=\"feature_eng2.jpeg\">\n",
    "\n",
    "\n",
    "* f_3 is a special feature with a very high variance. Compared to other features that have a low variance.Thus we would want to reduce the variance of this column, and that can be done by taking a log transformation.\n",
    "\n",
    "* The value of column f_3 ranges from 0 to 10000 and a histogram is shown below\n",
    "\n",
    "<img src=\"feature_eng3.jpeg\">\n",
    "\n",
    "\n",
    "And we apply log(1+x) to this column to reduce its variance. Figure 9 shows what happens to the histogram when log transformation is applied.\n",
    "<img src=\"feature_eng3.jpeg\">\n",
    "\n",
    "\n",
    "Sometimes, instead of log, you can also take exponential. A very interesting case is when you use a log-based evaluation metric, for example, RMSLE. In that case, you can train on log-transformed targets and convert back to original using exponential on the prediction. That would help optimize the model for the metric.\n",
    "\n",
    "\n",
    "When dealing with both categorical and numerical variables, you might encounter missing values. In previous chapter we saw different ways to handle missing values. But there are many ways to handle missing values. This is considered feature engineering.\n",
    "\n",
    "\n",
    "For categorical features, let’s keep it super simple. If you ever encounter missing values in categorical features, treat is as a new category! As simple as this is, it (almost) always works! \n",
    "\n",
    "\n",
    "One way to fill missing values in numerical data would be to choose a value that does not appear in the specific feature and fill using that. For example, let’s say 0 is not seen in the feature. So, we fill all the missing values using 0. This is one of the ways but might not be the most effective. One of the methods that works better than filling 0s for numerical data is to fill with mean instead. You can also try to fill with the median of all the values for that feature, or you can use the most common value to fill the missing values. There are just so many ways to do this. \n",
    "\n",
    "\n",
    "A fancy way of filling in the missing values would be to use a k-nearest neighbour method. You can select a sample with missing values and find the nearest neighbours utilising some kind of distance metric, for example, Euclidean distance. Then you can take the mean of all nearest neighbours and fill up the missing value. You can use the KNN imputer implementation for filling missing values like this.\n",
    "\n",
    "<img src=\"feature_eng5.jpeg\">\n",
    "\n",
    "Let's see how a matrix with missing values, as shown in the figure is handled by using **KNNImputer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11. ,  9. , 10. , 11. ,  6. ,  5. ],\n",
       "       [ 2. , 11. ,  5. ,  5. , 13. ,  7. ],\n",
       "       [ 5. ,  4. ,  1. , 10. ,  6. ,  5.5],\n",
       "       [ 4. ,  8. ,  2. ,  9. ,  4. ,  5.5],\n",
       "       [ 2. , 14. ,  8. ,  9. ,  7.5,  6. ],\n",
       "       [ 1. ,  5. ,  1. , 14. ,  1. ,  7. ],\n",
       "       [ 8. , 11. , 11. , 13. ,  9. ,  5. ],\n",
       "       [ 7.5,  7. ,  8. ,  8. ,  7. ,  4. ],\n",
       "       [ 7. ,  9. , 14. , 13. , 12. ,  2. ],\n",
       "       [ 5. , 11. ,  2. , 12. ,  2. ,  4. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import impute\n",
    "\n",
    "#create a random numpy array with 10 samples \n",
    "# and 6 features and values ranging from 1 to 15 \n",
    "\n",
    "x = np.random.randint(1,15,(10,6))\n",
    "\n",
    "#create the array to float \n",
    "x = x.astype(float)\n",
    "\n",
    "#randomly assign 10 elements to NAN\n",
    "x.ravel()[np.random.choice(x.size,10,replace = False)] = np.nan\n",
    "\n",
    "#use 3 nearest neighbours to fill na values \n",
    "knn_imputer = impute.KNNImputer(n_neighbors = 2)\n",
    "knn_imputer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of imputing missing values in a column would be to train a regression model that tries to predict missing values in a column based on other columns. So, you start with one column that has a missing value and treat this column as the target column for regression model without the missing values. Using all the other columns, you now train a model on samples for which there is no missing value in the concerned column and then try to predict target (the same column) for the samples that were removed earlier. This way, you have a more robust model based imputation. \n",
    "\n",
    "\n",
    "Now, let’s say you are working on a problem of predicting store sales of different items (per week or month). You have items, and you have store ids. So, you can create features like items per store. Now, this is one of the features that is not discussed above. These kinds of features cannot be generalized and come purely from domain, data and business knowledge. Look at the data and see what fits and create features accordingly.\n",
    "\n",
    "**Always remember to scale or normalize your features if you are using linear models like logistic regression or model like SVM. Tree based models will always work fine without any normalization of features.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
