{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approching text classification/regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, these problems are also known as Natural Language Processing (NLP) problems. NLP problems are also like images in the sense that, it’s quite different. You need to create pipelines you have never created before for tabular problems. There are many different types of NLP problems, and the most common type is the classification of strings. For computers, everything is numbers. Let’s say we start with a fundamental task of sentiment classification. We will try to classify sentiment from movie reviews. So, you have a text, and there is a sentiment associated with it. How will you approach this kind of problem? You start with the basics. One review maps to one target variable. A review is a bunch of sentences. So, until now you must have seen classifying only a single sentence, but in this problem, we will be classifying multiple sentences.\n",
    "\n",
    "\n",
    "### How would we start with the problem?\n",
    "\n",
    "A simple way is to create one list which contain all the positive words and another which contain all the negative words. If a sentence contain \n",
    "\n",
    "* Large number of positive words then it has positive reviews.\n",
    "* Large number of negative words then it has negative reviews.\n",
    "* If both equal number of negative and positive then neutral.\n",
    "\n",
    "This is one of the oldest way and the code is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(sentence, pos, neg):\n",
    "    \n",
    "    \"\"\" This function returns sentiment of sentence \n",
    "    :param sentence: sentence, a string \n",
    "    :param pos: set of positive words \n",
    "    :param neg: set of negative words \n",
    "    :return: returns positive, negative or neutral sentiment \"\"\" \n",
    "    \n",
    "    #split sentence by a space\n",
    "    #\"this is a sentence!\" becomes: \n",
    "    #[\"this\", \"is\" \"a\", \"sentence!\"] \n",
    "    #note that im splitting on all whitespaces \n",
    "    #if you want to split by space use .split(\"\") \n",
    "    sentence = sentence.split() \n",
    "    \n",
    "    #make sentence into a set \n",
    "    sentence = set(sentence) \n",
    "    #check number of common words with positive \n",
    "    num_common_pos = len(sentence.intersection(pos)) \n",
    "    \n",
    "    #check number of common words with negative \n",
    "    num_common_neg = len(sentence.intersection(neg)) \n",
    "    \n",
    "    #make conditions and return \n",
    "    #see how return used eliminates if else \n",
    "    \n",
    "    if num_common_pos > num_common_neg:\n",
    "        return \"positive\" \n",
    "    \n",
    "    if num_common_pos < num_common_neg:\n",
    "        return \"negative\" \n",
    "    \n",
    "    return \"neutral\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this kind of approach does not take a lot into consideration. And as you can see that our split() is also not perfect. If you use split(), a sentence like: \n",
    "\n",
    "“hi, how are you?” \n",
    "\n",
    "gets split into [“hi,”, “how”, “are”, “you?”] \n",
    "\n",
    "\n",
    "This is not ideal, because you see the comma and question mark, they are not split. It is therefore not recommended to use this method if you don’t have a preprocessing that handles these special characters before the split. Splitting a string into a list of words is known as tokenization. One of the most popular tokenization comes from **NLTK (Natural Language Tool Kit).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shubhangi/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi,', 'how', 'are', 'you?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"hi, how are you?\"\n",
    "\n",
    "\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', ',', 'how', 'are', 'you', '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using NLTK’s word tokenize, the same sentence is split in a much better manner. Comparing using a list of words will also work much better now! This is what we will apply to our first model to detect sentiment.\n",
    "\n",
    "One of the basic models that you should always try with a classification problem in NLP is bag of words. In bag of words, we create a huge sparse matrix that stores counts of all the words in our corpus (corpus = all the documents = all the sentences). For this, we will use CountVectorizer from scikit-learn. Let’s see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "#create a corpus of sentences \n",
    "corpus = [ \"hello, how are you?\", \n",
    "          \"im getting bored at home. And you? What do you think?\",\n",
    "          \"did you know about counts\", \"let's see if this works!\", \"YES!!!!\" ] \n",
    "\n",
    "#initialize CountVectorizer \n",
    "ctv = CountVectorizer() \n",
    "#fit the vectorizer on corpus \n",
    "ctv.fit(corpus) \n",
    "\n",
    "corpus_transformed = ctv.transform(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 22)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 22)\t2\n",
      "  (2, 0)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 22)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 20)\t1\n",
      "  (4, 21)\t1\n"
     ]
    }
   ],
   "source": [
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our corpus is now a sparse matrix, where, for first sample, we have four elements, for sample 2 we have ten elements, and so on, for sample 3 we have five elements and so on. We also see that these elements have a count associated with them. \n",
    "\n",
    "Some are seen twice, some are seen only once. For example, in sample 2 (row 1), we see that column 22 has a value of two. Why is that? And what is column 22? The way CountVectorizer works is it first tokenizes the sentence and then assigns a value to each token. So, each token is represented by a unique index. These unique indices are the columns that we see. The CountVectorizer stores this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 9, 'how': 11, 'are': 2, 'you': 22, 'im': 13, 'getting': 8, 'bored': 4, 'at': 3, 'home': 10, 'and': 1, 'what': 19, 'do': 7, 'think': 17, 'did': 6, 'know': 14, 'about': 0, 'counts': 5, 'let': 15, 'see': 16, 'if': 12, 'this': 18, 'works': 20, 'yes': 21}\n"
     ]
    }
   ],
   "source": [
    "print(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that index 22 belongs to “you” and in the second sentence, we have used “you” twice. Thus, the count is 2. I hope it’s clear now what is bag of words. But we are missing some special characters. Sometimes those special characters can be useful too. For example, “?” denotes a question in most sentences. Let’s integrate word_tokenize from scikit-learn in CountVectorizer and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "#create a corpus of sentences \n",
    "corpus = [ \"hello, how are you?\", \n",
    "          \"im getting bored at home. And you? What do you think?\", \n",
    "          \"did you know about counts\", \"let's see if this works!\", \n",
    "          \"YES!!!!\" ] \n",
    "\n",
    "#initialize CountVectorizer with word_tokenize from nltk \n",
    "#as the tokenizer \n",
    "\n",
    "ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None) \n",
    "\n",
    "#fit the vectorizer on corpus \n",
    "ctv.fit(corpus) \n",
    "\n",
    "corpus_transformed = ctv.transform(corpus) \n",
    "\n",
    "print(ctv.vocabulary_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have more words in the vocabulary. Thus, we can now create a sparse matrix by using all the sentences in IMDB dataset and can build a model. The ratio to positive and negative samples in this dataset is 1:1, and thus, we can use accuracy as the metric. \n",
    "\n",
    "We will use StratifiedKFold and create a single script to train five folds. Which model to use you ask? Which is the fastest model for high dimensional sparse data? Logistic regression. We will use logistic regression for this dataset to start with and to create our first actual benchmark.\n",
    "\n",
    "The code will be in the script **rg.py** and on running it we will get thee output as follows :\n",
    "\n",
    "<img src=\"../text_prob1.png\">\n",
    "\n",
    "\n",
    "* We will notice the following things when we run the code \n",
    "    * The accucracy is quite good for the first trial.\n",
    "    * We will also recive the warning message also and this is because the number of features (i.e the vocabulary) is much more in size as compare to the number of training examples.\n",
    "    \n",
    "    \n",
    "Now we will use another algorithm called MultinomialNB from scikit-learn.\n",
    "\n",
    "\n",
    "The code will be in the script **mb.py** and on running it we will get thee output as follows :\n",
    "\n",
    "\n",
    "<img src=\"../text_prob2.png\">\n",
    "\n",
    "We see that this score is low. But the naïve bayes model is superfast. \n",
    "Another method in NLP that most of the people these days tend to ignore or don’t care to know about is called **TF-IDF**. \n",
    "TF is term frequencies, and IDF is inverse document frequency. It might seem difficult from these terms, but things will become apparent with the formulae for TF and IDF. \n",
    "\n",
    "<img src=\"../ss1.jpeg\">\n",
    "\n",
    "\n",
    "\n",
    "Similar to CountVectorizer in scikit-learn, we have TfidfVectorizer. Let’s try using it the same way we used CountVectorizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 27)\t0.2965698850220162\n",
      "  (0, 16)\t0.4428321995085722\n",
      "  (0, 14)\t0.4428321995085722\n",
      "  (0, 7)\t0.4428321995085722\n",
      "  (0, 4)\t0.35727423026525224\n",
      "  (0, 2)\t0.4428321995085722\n",
      "  (1, 27)\t0.35299699146792735\n",
      "  (1, 24)\t0.2635440111190765\n",
      "  (1, 22)\t0.2635440111190765\n",
      "  (1, 18)\t0.2635440111190765\n",
      "  (1, 15)\t0.2635440111190765\n",
      "  (1, 13)\t0.2635440111190765\n",
      "  (1, 12)\t0.2635440111190765\n",
      "  (1, 9)\t0.2635440111190765\n",
      "  (1, 8)\t0.2635440111190765\n",
      "  (1, 6)\t0.2635440111190765\n",
      "  (1, 4)\t0.42525129752567803\n",
      "  (1, 3)\t0.2635440111190765\n",
      "  (2, 27)\t0.31752680284846835\n",
      "  (2, 19)\t0.4741246485558491\n",
      "  (2, 11)\t0.4741246485558491\n",
      "  (2, 10)\t0.4741246485558491\n",
      "  (2, 5)\t0.4741246485558491\n",
      "  (3, 25)\t0.38775666010579296\n",
      "  (3, 23)\t0.38775666010579296\n",
      "  (3, 21)\t0.38775666010579296\n",
      "  (3, 20)\t0.38775666010579296\n",
      "  (3, 17)\t0.38775666010579296\n",
      "  (3, 1)\t0.38775666010579296\n",
      "  (3, 0)\t0.3128396318588854\n",
      "  (4, 26)\t0.2959842226518677\n",
      "  (4, 0)\t0.9551928286692534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "#create a corpus of sentences \n",
    "corpus = [\"hello, how are you?\", \n",
    "          \"im getting bored at home. And you? What do you think?\",\n",
    "          \"did you know about counts\", \n",
    "          \"let's see if this works!\", \n",
    "          \"YES!!!!\" ] \n",
    "\n",
    "#initialize TfidfVectorizer with word_tokenize from nltk \n",
    "#as the tokenizer \n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None) \n",
    "#fit the vectorizer on corpus \n",
    "tfv.fit(corpus) \n",
    "corpus_transformed = tfv.transform(corpus) \n",
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will replace **CountVectorizer** with **TfidVectorizer** \n",
    "\n",
    "The code is in the script **rg1.py**  and on running it will get the output as follows:\n",
    "\n",
    "<img src=\"../text_prob2.png\">\n",
    "\n",
    "\n",
    "We see that these scores are a bit higher than CountVectorizer, and thus, it becomes the new benchmark that we would want to beat. \n",
    "Another interesting concept in NLP is n-grams. **N-grams** are combinations of words in order. N-grams are easy to create. You just need to take care of the order. To make things even more comfortable, we can use n-gram implementation from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', ',', 'how'), (',', 'how', 'are'), ('how', 'are', 'you'), ('are', 'you', '?')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams \n",
    "from nltk.tokenize import word_tokenize \n",
    "#let's see 3 grams \n",
    "N = 3 \n",
    "\n",
    "#input sentence \n",
    "sentence = \"hi, how are you?\" \n",
    "\n",
    "#tokenized sentence \n",
    "tokenized_sentence = word_tokenize(sentence) \n",
    "\n",
    "#generate n_grams \n",
    "n_grams = list(ngrams(tokenized_sentence, N)) \n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can also create 2-grams, or 4-grams, etc. Now, these n-grams become a part of our vocab, and when we calculate counts or tf-idf, we consider one n-gram as one entirely new token. So, in a way, we are incorporating context to some extent. Both CountVectorizer and TfidfVectorizer implementations of scikit-learn offers ngrams by ngram_range parameter, which has a minimum and maximum limit. By default, this is (1, 1). When we change it to (1, 3), we are looking at unigrams, bigrams and trigrams. The code change is minimal. Since we had the best result till now with tf-idf, let’s see if including n-grams up to trigrams improves the model. The only change required is in the initialization of TfidfVectorizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tdidf_vsc = TfidfVectorizer(\n",
    "                            tokenizer = word_tokenize,\n",
    "                            token_pattern = None,\n",
    "                            ngram_range= (1,3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we will improve in our previous code that is **rg1.py** and add the new code will we in **tvf_rg_trigram** and on running this we will get the following output \n",
    "\n",
    "<img src=\"../text_prob4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks okay, but we do not see any improvements. Probably you can try to do it on your own. There are a lot more things in the basics of NLP. One term that you must be aware of is **stemming.** Another is **lemmatization.** Stemming and lemmatization reduce a word to its smallest form. \n",
    "\n",
    "* In the case of stemming, the processed word is called the stemmed word, and in the case of lemmatization, it is known as the lemma. \n",
    "* It must be noted that lemmatization is more aggressive than stemming and stemming is more popular and widely used. \n",
    "* Both stemming and lemmatization come from linguistics. And you need to have an in-depth knowledge of a given language if you plan to make a stemmer or lemmatizer for that language.\n",
    "\n",
    "* Both stemming and lemmatization can be done easily by using the NLTK package.\n",
    "\n",
    "Below code contain the most common Snowball Stemmer and WordNet Lemmatizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=fishing\n",
      "stemmed_word=fish\n",
      "lemma=fishing\n",
      "\n",
      "word=fishes\n",
      "stemmed_word=fish\n",
      "lemma=fish\n",
      "\n",
      "word=fished\n",
      "stemmed_word=fish\n",
      "lemma=fished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "#initialize lemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#initialize stemmer \n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "words = [\"fishing\", \"fishes\", \"fished\"] \n",
    "for word in words:\n",
    "    print(f\"word={word}\") \n",
    "    print(f\"stemmed_word={stemmer.stem(word)}\") \n",
    "    print(f\"lemma={lemmatizer.lemmatize(word)}\") \n",
    "    print(\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When we do stemming, we are given the smallest form of a word which may or may not be a word in the dictionary for the language the word belongs to. \n",
    "* However, in the case of lemmatization, this will be a word.\n",
    "\n",
    "One more topic that you should be aware of is topic extraction. Topic extraction can be done using non-negative matrix factorization (NMF) or latent semantic analysis (LSA), which is also popularly known as singular value decomposition or SVD. These are decomposition techniques that reduce the data to a given number of components. You can fit any of these on sparse matrix obtained from CountVectorizer or TfidfVectorizer. Let’s apply it on TfidfVetorizer that we have used before.\n",
    "\n",
    "The code is in **svd.py** \n",
    "\n",
    "<img src=\"../text_prob5.png\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
